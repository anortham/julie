use anyhow::Result;
use rust_mcp_sdk::macros::mcp_tool;
use rust_mcp_sdk::macros::JsonSchema;
use rust_mcp_sdk::schema::{CallToolResult, TextContent};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tracing::{debug, info, warn};

use super::shared::OptimizedResponse;
use crate::embeddings::cosine_similarity;
use crate::extractors::{Symbol, SymbolKind};
use crate::handler::JulieServerHandler;
use crate::health::SystemReadiness;
use crate::utils::{
    context_truncation::ContextTruncator, exact_match_boost::ExactMatchBoost,
    path_relevance::PathRelevanceScorer, progressive_reduction::ProgressiveReducer,
    token_estimation::TokenEstimator,
};
use crate::workspace::registry_service::WorkspaceRegistryService;

//******************//
//   Search Tools   //
//******************//

#[mcp_tool(
    name = "fast_search",
    description = "SEARCH BEFORE CODING - Find existing implementations to avoid duplication with lightning speed",
    title = "Fast Unified Search (Text + Semantic)",
    idempotent_hint = true,
    destructive_hint = false,
    open_world_hint = false,
    read_only_hint = true,
    meta = r#"{"category": "search", "performance": "sub_10ms"}"#
)]
#[derive(Debug, Deserialize, Serialize, JsonSchema)]
pub struct FastSearchTool {
    /// Search query supporting multiple patterns and code constructs.
    /// Examples: "getUserData", "handle*", "class UserService", "import React", "TODO", "async function"
    /// Supports: exact match, wildcards (*), camelCase tokenization, partial matching
    pub query: String,
    /// Search algorithm: "text" (exact/pattern match, <10ms), "semantic" (AI similarity, <100ms), "hybrid" (both, balanced)
    /// Default: "text" for speed. Use "semantic" when text search fails to find conceptually similar code.
    /// Use "hybrid" for comprehensive results when you need maximum coverage.
    #[serde(default = "default_text")]
    pub mode: String,
    /// Programming language filter (optional).
    /// Valid: "rust", "typescript", "javascript", "python", "java", "csharp", "php", "ruby", "swift", "kotlin", "go", "c", "cpp", "lua", "sql", "html", "css", "vue", "bash", "gdscript", "dart", "zig"
    /// Example: "typescript" to search only .ts/.tsx files
    #[serde(default)]
    pub language: Option<String>,
    /// File path pattern using glob syntax (optional).
    /// Examples: "src/", "*.test.ts", "**/components/**", "tests/", "!node_modules/"
    /// Supports: directories, extensions, nested paths, exclusions with !
    #[serde(default)]
    pub file_pattern: Option<String>,
    /// Maximum results to return (default: 50, range: 1-500).
    /// Lower = faster response, Higher = more comprehensive
    /// Tip: Start with default, increase if you need more results
    #[serde(default = "default_limit")]
    pub limit: u32,
    /// Workspace filter (optional): "all" (search all workspaces), "primary" (primary only), or workspace ID
    /// Examples: "all", "primary", "project-b_a3f2b8c1"
    /// Default: "primary" - search only the primary workspace for focused results
    #[serde(default = "default_workspace")]
    pub workspace: Option<String>,
}

fn default_limit() -> u32 {
    50
}
fn default_text() -> String {
    "text".to_string()
}
fn default_workspace() -> Option<String> {
    Some("primary".to_string())
}

impl FastSearchTool {
    pub async fn call_tool(&self, handler: &JulieServerHandler) -> Result<CallToolResult> {
        debug!("üîç Fast search: {} (mode: {})", self.query, self.mode);

        // üöÄ NEW: Check system readiness with graceful degradation
        let readiness = crate::health::HealthChecker::check_system_readiness(handler).await?;

        match readiness {
            SystemReadiness::NotReady => {
                let message = "‚ùå Workspace not indexed yet!\nüí° Run 'manage_workspace index' first to enable fast search.";
                return Ok(CallToolResult::text_content(vec![TextContent::from(
                    message,
                )]));
            }
            SystemReadiness::SqliteOnly { symbol_count } => {
                // Graceful degradation: Use SQLite fallback
                warn!("üîÑ Search index building... using database search (slower but works!)");
                return self.fallback_sqlite_search(handler, symbol_count).await;
            }
            SystemReadiness::PartiallyReady {
                tantivy_ready,
                embeddings_ready,
                symbol_count,
            } => {
                // Show status but proceed with available systems
                let status_msg = format!(
                    "üîç Search {}% ready, embeddings {}% ready ({} symbols available)",
                    if tantivy_ready { 100 } else { 50 },
                    if embeddings_ready { 100 } else { 0 },
                    symbol_count
                );
                info!("{}", status_msg);
            }
            SystemReadiness::FullyReady { symbol_count } => {
                debug!("‚úÖ All systems ready ({} symbols)", symbol_count);
            }
        }

        // Perform search based on mode
        let symbols = match self.mode.as_str() {
            "semantic" => self.semantic_search(handler).await?,
            "hybrid" => self.hybrid_search(handler).await?,
            "text" | _ => self.text_search(handler).await?,
        };

        // Create optimized response with confidence scoring
        let confidence = self.calculate_search_confidence(&symbols);
        let mut optimized = OptimizedResponse::new(symbols, confidence);

        // Add insights based on patterns found
        if let Some(insights) = self.generate_search_insights(&optimized.results) {
            optimized = optimized.with_insights(insights);
        }

        // Add smart next actions
        let next_actions = self.suggest_next_actions(&optimized.results);
        optimized = optimized.with_next_actions(next_actions);

        // Optimize for tokens
        optimized.optimize_for_tokens(Some(self.limit as usize));

        if optimized.results.is_empty() {
            let message = format!(
                "üîç No results found for: '{}'\n\
                üí° Try a broader search term, different mode, or check spelling",
                self.query
            );
            return Ok(CallToolResult::text_content(vec![TextContent::from(
                message,
            )]));
        }

        // Format optimized results
        let message = self.format_optimized_results(&optimized);
        Ok(CallToolResult::text_content(vec![TextContent::from(
            message,
        )]))
    }

    async fn text_search(&self, handler: &JulieServerHandler) -> Result<Vec<Symbol>> {
        // Resolve workspace filtering
        let workspace_filter = self.resolve_workspace_filter(handler).await?;

        // If workspace filtering is specified, use database search for precise workspace isolation
        if let Some(workspace_ids) = workspace_filter {
            debug!(
                "üéØ Using workspace-filtered database search for workspace IDs: {:?}",
                workspace_ids
            );
            return self
                .database_search_with_workspace_filter(handler, workspace_ids)
                .await;
        }

        // For "all" workspaces, use the existing Tantivy search engine approach
        // Try to use persistent search engine from workspace first
        let search_results = if let Some(workspace) = handler.get_workspace().await? {
            if let Some(persistent_search) = &workspace.search {
                debug!("üöÄ Using persistent Tantivy search index");
                let search_engine = persistent_search.read().await;
                search_engine.search(&self.query).await.map_err(|e| {
                    debug!("Persistent search failed: {}", e);
                    anyhow::anyhow!("Persistent search failed: {}", e)
                })
            } else {
                debug!("‚ö†Ô∏è  No persistent search engine, using handler fallback");
                let search_engine = handler.active_search_engine().await;
                let search_engine = search_engine.read().await;
                search_engine.search(&self.query).await.map_err(|e| {
                    debug!("Handler search failed: {}", e);
                    anyhow::anyhow!("Handler search failed: {}", e)
                })
            }
        } else {
            debug!("‚ö†Ô∏è  No workspace initialized, using handler fallback");
            let search_engine = handler.active_search_engine().await;
            let search_engine = search_engine.read().await;
            search_engine.search(&self.query).await.map_err(|e| {
                debug!("Handler search failed: {}", e);
                anyhow::anyhow!("Handler search failed: {}", e)
            })
        };

        match search_results {
            Ok(results) => {
                // Use SearchResult symbols directly - no linear lookup needed!
                let mut matched_symbols = Vec::new();

                for search_result in results {
                    // Use the full Symbol from SearchResult directly - no linear lookup needed!
                    matched_symbols.push(search_result.symbol);
                }

                // Apply combined scoring: PathRelevanceScorer + ExactMatchBoost for optimal ranking
                let path_scorer = PathRelevanceScorer::new(&self.query);
                let exact_match_booster = ExactMatchBoost::new(&self.query);
                matched_symbols.sort_by(|a, b| {
                    // Combine path relevance (production vs test) with exact match boost
                    let path_score_a = path_scorer.calculate_score(&a.file_path);
                    let exact_boost_a = exact_match_booster.calculate_boost(&a.name);
                    let combined_score_a = path_score_a * exact_boost_a;

                    let path_score_b = path_scorer.calculate_score(&b.file_path);
                    let exact_boost_b = exact_match_booster.calculate_boost(&b.name);
                    let combined_score_b = path_score_b * exact_boost_b;

                    // Sort in descending order (higher combined scores first)
                    combined_score_b
                        .partial_cmp(&combined_score_a)
                        .unwrap_or(std::cmp::Ordering::Equal)
                });

                debug!("üöÄ Indexed search returned {} results (ranked by PathRelevanceScorer + ExactMatchBoost)", matched_symbols.len());
                Ok(matched_symbols)
            }
            Err(_) => {
                // Fallback to linear search if index fails
                warn!("‚ö†Ô∏è  Search engine failed, using linear search fallback");
                let symbols = handler.symbols.read().await;
                let query_lower = self.query.to_lowercase();

                let mut results: Vec<Symbol> = symbols
                    .iter()
                    .filter(|symbol| {
                        let name_match = symbol.name.to_lowercase().contains(&query_lower);
                        let language_match = self
                            .language
                            .as_ref()
                            .map(|lang| symbol.language.eq_ignore_ascii_case(lang))
                            .unwrap_or(true);
                        name_match && language_match
                    })
                    .cloned()
                    .collect();

                // Apply combined scoring: PathRelevanceScorer + ExactMatchBoost for optimal ranking
                let path_scorer = PathRelevanceScorer::new(&self.query);
                let exact_match_booster = ExactMatchBoost::new(&self.query);
                results.sort_by(|a, b| {
                    // Combine path relevance (production vs test) with exact match boost
                    let path_score_a = path_scorer.calculate_score(&a.file_path);
                    let exact_boost_a = exact_match_booster.calculate_boost(&a.name);
                    let combined_score_a = path_score_a * exact_boost_a;

                    let path_score_b = path_scorer.calculate_score(&b.file_path);
                    let exact_boost_b = exact_match_booster.calculate_boost(&b.name);
                    let combined_score_b = path_score_b * exact_boost_b;

                    // Sort in descending order (higher combined scores first)
                    combined_score_b
                        .partial_cmp(&combined_score_a)
                        .unwrap_or(std::cmp::Ordering::Equal)
                });

                debug!("üìù Linear search fallback returned {} results (ranked by PathRelevanceScorer + ExactMatchBoost)", results.len());
                Ok(results)
            }
        }
    }

    async fn semantic_search(&self, handler: &JulieServerHandler) -> Result<Vec<Symbol>> {
        debug!("üß† Semantic search mode (using embeddings)");

        // CRITICAL FIX: Get ALL symbols for semantic comparison (not just text matches!)
        // This enables true conceptual search using embeddings
        let all_symbols = {
            let symbols = handler.symbols.read().await;

            // Apply basic filters (language, file pattern) if specified
            let filtered: Vec<Symbol> = symbols
                .iter()
                .filter(|symbol| {
                    // Apply language filter if specified
                    let language_match = self
                        .language
                        .as_ref()
                        .map(|lang| symbol.language.eq_ignore_ascii_case(lang))
                        .unwrap_or(true);

                    // Apply file pattern filter if specified (basic contains check for now)
                    let file_match = self
                        .file_pattern
                        .as_ref()
                        .map(|pattern| {
                            if pattern.starts_with('!') {
                                // Exclusion pattern
                                !symbol.file_path.contains(&pattern[1..])
                            } else {
                                // Inclusion pattern
                                symbol.file_path.contains(pattern)
                            }
                        })
                        .unwrap_or(true);

                    language_match && file_match
                })
                .cloned()
                .collect();

            // Limit to reasonable number for performance (expand search space vs text search)
            let semantic_limit = (self.limit * 10).min(1000) as usize;
            filtered
                .into_iter()
                .take(semantic_limit)
                .collect::<Vec<Symbol>>()
        };

        debug!("üîç Semantic search evaluating {} symbols (vs text search which only evaluates exact matches)", all_symbols.len());

        // Ensure embedding engine is initialized
        handler.ensure_embedding_engine().await?;

        // Check embedding status for better user feedback
        let workspace = handler.get_workspace().await?;
        let embeddings_ready = workspace
            .as_ref()
            .map(|_| {
                // TODO: Check workspace registry for embedding status
                // For now, assume embeddings might still be generating
                false // Placeholder - should check actual embedding status
            })
            .unwrap_or(false);

        // Get mutable access to embedding engine for embedding generation
        let mut embedding_guard = handler.embedding_engine.write().await;
        let embedding_engine = match embedding_guard.as_mut() {
            Some(engine) => engine,
            None => {
                if !embeddings_ready {
                    info!("üîÑ Embeddings still generating in background - falling back to basic text matching");
                } else {
                    debug!("No embedding engine available, falling back to basic text matching");
                }
                // Fallback: basic text similarity without embeddings
                let query_lower = self.query.to_lowercase();
                let mut text_matches: Vec<Symbol> = all_symbols
                    .into_iter()
                    .filter(|symbol| {
                        symbol.name.to_lowercase().contains(&query_lower)
                            || symbol
                                .signature
                                .as_ref()
                                .map_or(false, |sig| sig.to_lowercase().contains(&query_lower))
                    })
                    .collect();
                text_matches.truncate(self.limit as usize);
                return Ok(text_matches);
            }
        };

        // Generate embedding for the query
        let query_embedding = {
            // Create a temporary symbol from the query for embedding
            let query_symbol = Symbol {
                id: "query".to_string(),
                name: self.query.clone(),
                kind: crate::extractors::base::SymbolKind::Function, // Arbitrary kind for query
                language: "query".to_string(),
                file_path: "query".to_string(),
                start_line: 1,
                start_column: 0,
                end_line: 1,
                end_column: self.query.len() as u32,
                start_byte: 0,
                end_byte: self.query.len() as u32,
                signature: None,
                doc_comment: None,
                visibility: None,
                parent_id: None,
                metadata: None,
                semantic_group: None,
                confidence: None,
                code_context: None,
            };

            let context = crate::embeddings::CodeContext {
                parent_symbol: None,
                surrounding_code: None,
                file_context: Some("".to_string()),
            };

            embedding_engine.embed_symbol(&query_symbol, &context)?
        };

        // PERFORMANCE OPTIMIZATION: Use batch embedding instead of individual calls
        let mut scored_symbols = Vec::new();

        if all_symbols.is_empty() {
            debug!("No symbols available for semantic search");
            return Ok(Vec::new());
        }

        // Generate embeddings for all candidates in one batch call
        match embedding_engine.embed_symbols_batch(&all_symbols) {
            Ok(batch_results) => {
                // Create a map for efficient lookup of embeddings by symbol ID
                let embedding_map: HashMap<String, Vec<f32>> = batch_results.into_iter().collect();

                // Calculate similarities for all symbols
                for symbol in all_symbols {
                    if let Some(embedding) = embedding_map.get(&symbol.id) {
                        let similarity = cosine_similarity(&query_embedding, embedding);
                        scored_symbols.push((symbol, similarity));
                    } else {
                        // Symbol didn't get embedded, use text similarity fallback
                        let text_similarity = if symbol
                            .name
                            .to_lowercase()
                            .contains(&self.query.to_lowercase())
                        {
                            0.8
                        } else if symbol.name.to_lowercase() == self.query.to_lowercase() {
                            1.0
                        } else {
                            0.3
                        };
                        scored_symbols.push((symbol, text_similarity));
                    }
                }
            }
            Err(e) => {
                debug!(
                    "Batch embedding failed: {}, falling back to individual processing",
                    e
                );

                // Fallback to individual processing if batch fails
                for symbol in all_symbols {
                    let context = crate::embeddings::CodeContext {
                        parent_symbol: None,
                        surrounding_code: symbol.code_context.clone(),
                        file_context: Some(symbol.signature.clone().unwrap_or_default()),
                    };

                    match embedding_engine.embed_symbol(&symbol, &context) {
                        Ok(embedding) => {
                            let similarity = cosine_similarity(&query_embedding, &embedding);
                            scored_symbols.push((symbol, similarity));
                        }
                        Err(e) => {
                            debug!("Failed to embed symbol {}: {}", symbol.name, e);
                            // Fall back to text similarity if embedding fails
                            let text_similarity = if symbol
                                .name
                                .to_lowercase()
                                .contains(&self.query.to_lowercase())
                            {
                                0.8
                            } else if symbol.name.to_lowercase() == self.query.to_lowercase() {
                                1.0
                            } else {
                                0.3
                            };
                            scored_symbols.push((symbol, text_similarity));
                        }
                    }
                }
            }
        }

        // Sort by similarity score (descending)
        scored_symbols.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

        // Return top results
        let results: Vec<Symbol> = scored_symbols
            .into_iter()
            .take(self.limit as usize)
            .map(|(symbol, _score)| symbol)
            .collect();

        debug!("Semantic search returned {} results", results.len());
        Ok(results)
    }

    async fn hybrid_search(&self, handler: &JulieServerHandler) -> Result<Vec<Symbol>> {
        // For now, delegate to text search - full hybrid implementation coming soon
        debug!("üîÑ Hybrid search mode (using text fallback)");
        self.text_search(handler).await
    }

    /// Calculate confidence score based on search quality and result relevance
    fn calculate_search_confidence(&self, symbols: &[Symbol]) -> f32 {
        if symbols.is_empty() {
            return 0.0;
        }

        let mut confidence: f32 = 0.5; // Base confidence

        // Exact name matches boost confidence
        let exact_matches = symbols
            .iter()
            .filter(|s| s.name.to_lowercase() == self.query.to_lowercase())
            .count();
        if exact_matches > 0 {
            confidence += 0.3;
        }

        // Partial matches are medium confidence
        let partial_matches = symbols
            .iter()
            .filter(|s| s.name.to_lowercase().contains(&self.query.to_lowercase()))
            .count();
        if partial_matches > exact_matches {
            confidence += 0.2;
        }

        // More results can indicate ambiguity (lower confidence)
        if symbols.len() > 20 {
            confidence -= 0.1;
        } else if symbols.len() < 5 {
            confidence += 0.1;
        }

        confidence.clamp(0.0, 1.0)
    }

    /// Generate intelligent insights about search patterns
    fn generate_search_insights(&self, symbols: &[Symbol]) -> Option<String> {
        if symbols.is_empty() {
            return None;
        }

        let mut insights = Vec::new();

        // Language distribution
        let mut lang_counts = HashMap::new();
        for symbol in symbols {
            *lang_counts.entry(&symbol.language).or_insert(0) += 1;
        }

        if lang_counts.len() > 1 {
            let main_lang = lang_counts.iter().max_by_key(|(_, count)| *count).unwrap();
            insights.push(format!(
                "Found across {} languages (mainly {})",
                lang_counts.len(),
                main_lang.0
            ));
        }

        // Kind distribution
        let mut kind_counts = HashMap::new();
        for symbol in symbols {
            *kind_counts.entry(&symbol.kind).or_insert(0) += 1;
        }

        if let Some((dominant_kind, count)) = kind_counts.iter().max_by_key(|(_, count)| *count) {
            if *count > symbols.len() / 2 {
                insights.push(format!(
                    "Mostly {:?}s ({} of {})",
                    dominant_kind,
                    count,
                    symbols.len()
                ));
            }
        }

        if insights.is_empty() {
            None
        } else {
            Some(insights.join(", "))
        }
    }

    /// Suggest intelligent next actions based on search results
    fn suggest_next_actions(&self, symbols: &[Symbol]) -> Vec<String> {
        let mut actions = Vec::new();

        if symbols.len() == 1 {
            actions.push("Use fast_goto to jump to definition".to_string());
            actions.push("Use fast_refs to see all usages".to_string());
        } else if symbols.len() > 1 {
            actions.push("Narrow search with language filter".to_string());
            actions.push("Use fast_refs on specific symbols".to_string());
        }

        // Check if we have functions that might be entry points
        if symbols
            .iter()
            .any(|s| matches!(s.kind, SymbolKind::Function) && s.name.contains("main"))
        {
            actions.push("Use fast_explore to understand architecture".to_string());
        }

        if symbols
            .iter()
            .any(|s| s.name.to_lowercase().contains(&self.query.to_lowercase()))
        {
            actions.push("Consider exact name match for precision".to_string());
        }

        actions
    }

    /// Format optimized response with insights and next actions
    pub fn format_optimized_results(&self, optimized: &OptimizedResponse<Symbol>) -> String {
        let mut lines = vec![format!(
            "‚ö° Fast Search: '{}' (mode: {})",
            self.query, self.mode
        )];

        // Add insights if available
        if let Some(insights) = &optimized.insights {
            lines.push(format!("üí° {}", insights));
        }

        lines.push(String::new());

        // Token optimization: apply progressive reduction first, then early termination if needed
        let token_estimator = TokenEstimator::new();
        let token_limit: usize = 15000; // 15K token limit to stay within Claude's context window
        let progressive_reducer = ProgressiveReducer::new();

        // Calculate initial header tokens
        let header_text = lines.join("\n");
        let header_tokens = token_estimator.estimate_string(&header_text);
        let available_tokens = token_limit.saturating_sub(header_tokens);

        // Define token estimator function for symbols
        let estimate_symbols_tokens = |symbols: &[&Symbol]| -> usize {
            let mut total_tokens = 0;
            for (i, symbol) in symbols.iter().enumerate() {
                let mut symbol_text = String::new();
                symbol_text.push_str(&format!(
                    "{}. {} [{}]\n",
                    i + 1,
                    symbol.name,
                    symbol.language
                ));
                symbol_text.push_str(&format!(
                    "   üìÅ {}:{}-{}\n",
                    symbol.file_path, symbol.start_line, symbol.end_line
                ));

                if let Some(signature) = &symbol.signature {
                    symbol_text.push_str(&format!("   üìù {}\n", signature));
                }

                if let Some(context) = &symbol.code_context {
                    symbol_text.push_str("   üìÑ Context:\n");
                    let context_lines: Vec<String> =
                        context.lines().map(|s| s.to_string()).collect();
                    let max_lines = 10;

                    if context_lines.len() > max_lines {
                        let truncated_lines: Vec<String> =
                            context_lines.iter().take(max_lines).cloned().collect();
                        let lines_truncated = context_lines.len() - max_lines;
                        for context_line in &truncated_lines {
                            symbol_text.push_str(&format!("   {}\n", context_line));
                        }
                        symbol_text
                            .push_str(&format!("   ({} more lines truncated)\n", lines_truncated));
                    } else {
                        for context_line in &context_lines {
                            symbol_text.push_str(&format!("   {}\n", context_line));
                        }
                    }
                }

                total_tokens += token_estimator.estimate_string(&symbol_text);
            }
            total_tokens
        };

        // Try progressive reduction first
        let symbol_refs: Vec<&Symbol> = optimized.results.iter().collect();
        let reduced_symbol_refs =
            progressive_reducer.reduce(&symbol_refs, available_tokens, estimate_symbols_tokens);

        let (symbols_to_show, reduction_message) = if reduced_symbol_refs.len()
            < optimized.results.len()
        {
            // Progressive reduction was applied
            let symbols: Vec<Symbol> = reduced_symbol_refs.into_iter().cloned().collect();
            let message = format!("üìä Showing {} of {} results (confidence: {:.1}) - Applied progressive reduction {} ‚Üí {}",
                    symbols.len(), optimized.total_found, optimized.confidence, optimized.results.len(), symbols.len());
            (symbols, message)
        } else {
            // No reduction needed
            let message = format!(
                "üìä Showing {} of {} results (confidence: {:.1})",
                optimized.results.len(),
                optimized.total_found,
                optimized.confidence
            );
            (optimized.results.clone(), message)
        };

        lines[1] = reduction_message;

        // Format the symbols we decided to show
        for (i, symbol) in symbols_to_show.iter().enumerate() {
            lines.push(format!("{}. {} [{}]", i + 1, symbol.name, symbol.language));
            lines.push(format!(
                "   üìÅ {}:{}-{}",
                symbol.file_path, symbol.start_line, symbol.end_line
            ));

            if let Some(signature) = &symbol.signature {
                lines.push(format!("   üìù {}", signature));
            }

            // Add code context if available
            if let Some(context) = &symbol.code_context {
                lines.push("   üìÑ Context:".to_string());

                // Apply context truncation using ContextTruncator
                let truncator = ContextTruncator::new();
                let context_lines: Vec<String> = context.lines().map(|s| s.to_string()).collect();
                let max_lines = 10; // Max 10 lines per symbol

                if context_lines.len() > max_lines {
                    // Truncate and show truncation message
                    let truncated_lines = truncator.truncate_lines(&context_lines, max_lines);
                    let lines_truncated = context_lines.len() - max_lines;

                    // Add truncated lines
                    for context_line in &truncated_lines {
                        lines.push(format!("   {}", context_line));
                    }

                    // Add truncation message
                    lines.push(format!("   ({} more lines truncated)", lines_truncated));
                } else {
                    // Add all lines if within limit
                    for context_line in &context_lines {
                        lines.push(format!("   {}", context_line));
                    }
                }
            }

            lines.push(String::new());
        }

        // Add next actions
        if !optimized.next_actions.is_empty() {
            lines.push("üéØ Suggested next actions:".to_string());
            for action in &optimized.next_actions {
                lines.push(format!("   ‚Ä¢ {}", action));
            }
        }

        lines.join("\n")
    }

    /// Resolve workspace filtering parameter to a list of workspace IDs
    async fn resolve_workspace_filter(
        &self,
        handler: &JulieServerHandler,
    ) -> Result<Option<Vec<String>>> {
        let workspace_param = self.workspace.as_deref().unwrap_or("primary");

        match workspace_param {
            "all" => {
                // Search across all workspaces - return None to indicate no filtering
                Ok(None)
            }
            "primary" => {
                // FIX: Use Tantivy search for primary workspace to support multi-word queries
                // Previously this used database search with workspace filtering, but that
                // routes to LIKE '%pattern%' which can't handle space-separated terms.
                // Since primary workspace is the main workspace, using Tantivy provides
                // better search capabilities (multi-word, fuzzy, semantic) without loss of precision.
                Ok(None)
            }
            workspace_id => {
                // Validate the workspace ID exists
                if let Some(primary_workspace) = handler.get_workspace().await? {
                    let registry_service =
                        WorkspaceRegistryService::new(primary_workspace.root.clone());

                    // Check if it's a valid workspace ID
                    match registry_service.get_workspace(workspace_id).await? {
                        Some(_) => Ok(Some(vec![workspace_id.to_string()])),
                        None => {
                            // Invalid workspace ID
                            return Err(anyhow::anyhow!(
                                "Workspace '{}' not found. Use 'all', 'primary', or a valid workspace ID",
                                workspace_id
                            ));
                        }
                    }
                } else {
                    return Err(anyhow::anyhow!(
                        "No primary workspace found. Initialize workspace first."
                    ));
                }
            }
        }
    }

    /// Perform database search with workspace filtering for precise workspace isolation
    async fn database_search_with_workspace_filter(
        &self,
        handler: &JulieServerHandler,
        workspace_ids: Vec<String>,
    ) -> Result<Vec<Symbol>> {
        let workspace = handler
            .get_workspace()
            .await?
            .ok_or_else(|| anyhow::anyhow!("No workspace initialized"))?;

        let db = workspace
            .db
            .as_ref()
            .ok_or_else(|| anyhow::anyhow!("No database available"))?;

        let db_lock = db.lock().await;

        // Use the workspace-aware database search
        let mut results =
            db_lock.find_symbols_by_pattern(&self.query, Some(workspace_ids.clone()))?;

        // Apply language filtering if specified
        if let Some(ref language) = self.language {
            results.retain(|symbol| symbol.language.eq_ignore_ascii_case(language));
        }

        // Apply file pattern filtering if specified
        if let Some(ref pattern) = self.file_pattern {
            results.retain(|symbol| {
                // Simple glob pattern matching - could be enhanced
                let file_path = &symbol.file_path;
                if pattern.contains('*') {
                    let pattern_parts: Vec<&str> = pattern.split('*').collect();
                    if pattern_parts.len() == 2 {
                        file_path.starts_with(pattern_parts[0])
                            && file_path.ends_with(pattern_parts[1])
                    } else {
                        file_path.contains(&pattern.replace('*', ""))
                    }
                } else {
                    file_path.contains(pattern)
                }
            });
        }

        // Apply combined scoring and sorting
        let path_scorer = PathRelevanceScorer::new(&self.query);
        let exact_match_booster = ExactMatchBoost::new(&self.query);
        results.sort_by(|a, b| {
            let path_score_a = path_scorer.calculate_score(&a.file_path);
            let exact_boost_a = exact_match_booster.calculate_boost(&a.name);
            let combined_score_a = path_score_a * exact_boost_a;

            let path_score_b = path_scorer.calculate_score(&b.file_path);
            let exact_boost_b = exact_match_booster.calculate_boost(&b.name);
            let combined_score_b = path_score_b * exact_boost_b;

            combined_score_b
                .partial_cmp(&combined_score_a)
                .unwrap_or(std::cmp::Ordering::Equal)
        });

        // Apply limit
        if results.len() > self.limit as usize {
            results.truncate(self.limit as usize);
        }

        debug!(
            "üóÑÔ∏è Database search with workspace filter returned {} results",
            results.len()
        );
        Ok(results)
    }

    /// üîÑ Graceful degradation: SQLite-based search when Tantivy isn't ready
    async fn fallback_sqlite_search(
        &self,
        handler: &JulieServerHandler,
        symbol_count: i64,
    ) -> Result<CallToolResult> {
        info!(
            "üîÑ Using SQLite fallback search ({} symbols available)",
            symbol_count
        );

        // Use database search directly
        let symbols = self
            .database_search_with_workspace_filter(handler, vec!["primary".to_string()])
            .await?;

        if symbols.is_empty() {
            let message = format!(
                "üîç No results found for: '{}' (using database search while index builds)\n\
                üí° Try a broader search term or wait for search index to complete",
                self.query
            );
            return Ok(CallToolResult::text_content(vec![TextContent::from(
                message,
            )]));
        }

        // Create basic response (without advanced optimizations)
        let mut response_lines = vec![
            format!("üîÑ Search results (database fallback - index building in background):"),
            format!("üìä Found {} matches for '{}'", symbols.len(), self.query),
            String::new(),
        ];

        for (i, symbol) in symbols.iter().enumerate() {
            if i >= 20 {
                // Limit for fallback mode
                response_lines.push(format!("   ... and {} more results", symbols.len() - i));
                break;
            }

            response_lines.push(format!(
                "{}. {} ({:?}) - {}:{}",
                i + 1,
                symbol.name,
                symbol.kind,
                symbol.file_path,
                symbol.start_line
            ));

            if let Some(signature) = &symbol.signature {
                response_lines.push(format!("   üìù {}", signature));
            }
        }

        response_lines.extend(vec![
            String::new(),
            "üéØ Suggested actions:".to_string(),
            "   ‚Ä¢ Wait for search index to complete for faster results".to_string(),
            "   ‚Ä¢ Use fast_goto to jump to specific symbols".to_string(),
            "   ‚Ä¢ Try more specific search terms".to_string(),
        ]);

        let message = response_lines.join("\n");
        Ok(CallToolResult::text_content(vec![TextContent::from(
            message,
        )]))
    }
}
