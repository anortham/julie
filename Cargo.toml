[workspace]
members = [".", "crates/julie-extractors"]
resolver = "2"

[workspace.package]
edition = "2024"
authors = ["Alan"]
license = "MIT"

[package]
name = "julie"
version = "1.19.0"
edition.workspace = true
authors.workspace = true
description = "Julie - Cross-Platform Code Intelligence Server"
license.workspace = true

[lib]
name = "julie"
path = "src/lib.rs"

[features]
default = []
network_models = []

[[bin]]
name = "julie-server"
path = "src/main.rs"

[[bin]]
name = "julie-semantic"
path = "src/bin/semantic.rs"

[[bin]]
name = "julie-codesearch"
path = "src/bin/codesearch.rs"


[dependencies]
# Julie Extractors (workspace member crate)
julie-extractors = { path = "crates/julie-extractors" }

# MCP Server SDK
rust-mcp-sdk = { version = "0.7.0", default-features = false, features = [
    "server",
    "macros",
    "stdio",
    "2025_06_18",
] }

# Async recursion support
async-recursion = "1.1"

# Tree-sitter core (for tools/refactoring and tools/workspace/indexing)
# Language-specific parsers moved to julie-extractors crate
tree-sitter = "0.25"

# Embeddings - Direct ONNX Runtime with GPU acceleration
# Platform-specific dependencies added below for GPU support
# fastembed = "5.2"  # REMOVED - no GPU support

# Tokenizer for BERT-based embeddings (BGE models)
# Disable esaxx_fast to avoid the C++ static CRT build on Windows debug builds.
tokenizers = { version = "0.20", default-features = false, features = ["progressbar", "onig"] }

# HuggingFace Hub for model downloading
hf-hub = { version = "0.4", features = ["tokio"] }

# HNSW for fast vector similarity search (semantic code search)
hnsw_rs = "0.3"

# N-dimensional arrays for ONNX Runtime tensors
ndarray = "0.16"

# Async runtime
tokio = { version = "1.47.1", features = ["full"] }

# Parallelism
rayon = "1.10"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.8"

# Database
rusqlite = { version = "0.37", features = ["bundled"] }

# File watching
notify = "8.2"

# Utilities
async-trait = "0.1"
futures = "0.3"
diff-match-patch-rs = "0.5.1"  # Google's proven diff/patch algorithm for professional editing
uuid = { version = "1.10", features = ["v4", "serde"] }
thiserror = "2.0"
anyhow = "1.0"
md5 = "0.7"
regex = "1.11"
blake3 = "1.8"
hex = "0.4"
shellexpand = "3.1"
sha2 = "0.10"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
tracing-appender = "0.2"

# Pattern matching for file discovery
glob = "0.3"
globset = "0.4.16"  # Advanced glob matching with exclusion patterns support
chrono = { version = "0.4.42", features = ["serde"] }

# CLI-specific dependencies
clap = { version = "4.5", features = ["derive"] }
walkdir = "2.5"
num_cpus = "1.16"
toon-format = "0.3.7"

[dev-dependencies]
tempfile = "3.13"
tokio-test = "0.4"
cargo-tarpaulin = "0.31"
serial_test = "3.2"

[profile.release]
opt-level = 3
debug = false
strip = true
lto = true
codegen-units = 1
panic = "abort"

[profile.dev]
opt-level = 0
debug = true

# Platform-specific ONNX Runtime dependencies for GPU acceleration
# Each platform gets optimized execution providers for maximum performance

[target.'cfg(target_os = "windows")'.dependencies]
# DirectML - Windows GPU acceleration (works with NVIDIA, AMD, Intel GPUs)
ort = { version = "2.0.0-rc.10", features = ["directml", "download-binaries", "ndarray"] }
# Windows APIs for GPU enumeration via DXGI
windows = { version = "0.58", features = ["Win32_Graphics_Dxgi", "Win32_Graphics_Direct3D12"] }

[target.'cfg(all(target_os = "linux", target_arch = "x86_64"))'.dependencies]
# CUDA - NVIDIA GPU acceleration on Linux (TensorRT disabled due to CUDA version mismatch)
ort = { version = "2.0.0-rc.10", features = ["cuda", "download-binaries", "copy-dylibs", "ndarray"] }

[target.'cfg(target_os = "macos")'.dependencies]
# CoreML - Apple Silicon Neural Engine acceleration
ort = { version = "2.0.0-rc.10", features = ["coreml", "download-binaries", "ndarray"] }
