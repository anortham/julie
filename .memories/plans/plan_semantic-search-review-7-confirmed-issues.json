{
  "id": "plan_semantic-search-review-7-confirmed-issues",
  "timestamp": 1763066081,
  "type": "plan",
  "title": "Semantic Search Review: 7 Confirmed Issues",
  "status": "active",
  "content": "## Overview\nExternal review of semantic search/embeddings found 7 issues. Verified all claims systematically.\n\n## Phase 1: Correctness Bugs (Ship-Blockers)\n\n### üî¥ Issue #1: Result Ordering Nondeterminism (CRITICAL)\n**Location:** `src/database/symbols/queries.rs:24-51`\n**Bug:** `get_symbols_by_ids()` uses `WHERE id IN()` with no `ORDER BY` clause\n**Impact:** SQLite returns rows in arbitrary order, causing similarity scores to be paired with wrong symbols\n**Severity:** HIGH - Semantic search returns garbage results intermittently\n**Fix:** Add `ORDER BY` with position preservation (CASE statement or HashMap remapping)\n**Status:** Pending\n\n### üî¥ Issue #2: N√ó2 Database Hits Per Query (PERFORMANCE CLIFF)\n**Location:** `src/embeddings/loaded_index.rs:192-210` + `src/database/embeddings.rs:177-197`\n**Bug:** `get_embedding_for_symbol()` runs 2 SQL queries per neighbor\n  - Query 1: `SELECT vector_id FROM embeddings WHERE symbol_id = ?1`\n  - Query 2: `SELECT vector_data FROM embedding_vectors WHERE vector_id = ?1`\n**Impact:** Default limit=200 ‚Üí **400 database queries per search!**\n**Severity:** HIGH - 10-20x slower than necessary\n**Fix:** Batch fetch all vector_ids with `IN (...)`, then join via HashMap\n**Status:** Pending\n\n### üî¥ Issue #3: No Vector Length Validation\n**Location:** `src/database/embeddings.rs:137-148`\n**Bug:** `bulk_store_embeddings()` never validates `vector_data.len() == dimensions`\n**Impact:** Truncated vectors stored silently, later search panics on dimension mismatch\n**Severity:** MEDIUM - Time bomb that crashes production\n**Fix:** Add validation: `if vector_data.len() != dimensions { return Err(...) }`\n**Status:** Pending\n\n## Phase 2: Architecture Issues (Design Improvements)\n\n### ‚ö†Ô∏è Issue #4: Vector ID Collisions Across Models\n**Location:** `src/database/embeddings.rs:141-153` + `src/database/schema.rs:475`\n**Bug:** Using `symbol_id` as PRIMARY KEY `vector_id` overwrites when switching models\n**Impact:** Can't A/B test models or run side-by-side comparisons\n**Severity:** MEDIUM - Limits flexibility\n**Fix:** Use composite key: `{symbol_id}:{model_name}` as vector_id\n**Status:** Pending\n\n### ‚ö†Ô∏è Issue #5: Hard-Coded Dimensions\n**Location:** `src/embeddings/loaded_index.rs:174` (and others)\n**Bug:** Magic number `384` hard-coded in multiple files\n**Impact:** Switching to 768-dim model requires hunting down every occurrence\n**Severity:** MEDIUM - Fragile code\n**Fix:** Pass dimensions from `VectorStore::get_dimensions()` or embedding config\n**Status:** Pending\n\n## Phase 3: Performance Optimizations (Nice-to-Haves)\n\n### üí° Issue #6: CPU-Heavy Serialization\n**Location:** `src/database/embeddings.rs:139`\n**Bug:** `vector_data.iter().flat_map(|f| f.to_le_bytes()).collect()` creates many small allocations\n**Impact:** Slower bulk inserts (not critical)\n**Severity:** LOW\n**Fix:** Use `bytemuck::cast_slice()` for single memcpy\n**Status:** Pending\n\n## REJECTED: Issue #7 - Context Parameter Unused\n\n### ‚ùå Issue #7: Context Parameter Unused (INTENTIONAL DESIGN)\n**Location:** `src/embeddings/mod.rs:188-193`\n**Claim:** `embed_symbol()` accepts `_context: &CodeContext` but ignores it\n**Reviewer Recommendation:** Thread context through to embeddings\n\n**DECISION: REJECT - Evidence-based optimization**\n\n**Background (TODO.md Priority 0):**\nWe ALREADY tried including code_context in embeddings (2025-11-11):\n\n**Results of Including Context:**\n- Database: 50MB ‚Üí 203MB (4x growth)\n- Embedding generation: Significantly slower\n- 88% of embedding text was code_context (1,304 chars avg)\n- BGE-Small truncates at 512 tokens (~2KB), so most context got truncated anyway!\n- Test fixtures: 479KB code_context fields (99.6% truncated waste)\n\n**Phase 1 Fix: Remove code_context**\n- ‚úÖ Removed from `build_embedding_text()`\n- ‚úÖ Result: **75-88% faster embedding generation**\n- ‚úÖ Result: **Search quality MAINTAINED** (no loss!)\n- ‚úÖ RAG Principle: Embed semantic units (one concept), not random surrounding code\n\n**Conclusion:**\nThe `_context` parameter exists as API for potential future experiments, but using it would revert a proven optimization. The underscore prefix (`_context`) is Rust's way of marking \"intentionally unused\" while preserving the function signature.\n\n**Action:** Keep parameter unused, update documentation to explain the decision\n\n## Implementation Order\n\n1. **Fix #1 (ordering)** - Prevents wrong results ‚Üê START HERE\n2. **Fix #2 (N√ó2 queries)** - Major performance win\n3. **Fix #3 (validation)** - Prevents crashes\n4. **Fix #4 (collisions)** - Architecture flexibility\n5. **Fix #5 (dimensions)** - Code maintainability\n6. **Fix #6 (serialization)** - Micro-optimization\n7. **Issue #7 (context)** - Document decision, no code change\n\n## Confidence Assessment\n- Reviewer **knows their stuff** - 6 of 7 issues are real bugs\n- Issue #1 (ordering) is already shipping and affecting production users\n- Issue #2 (N√ó2 queries) explains why semantic search feels slow\n- All fixes are achievable without major architecture changes\n\n## Next Steps\n1. Create TDD tests for Issue #1 (ordering bug)\n2. Implement fix with ORDER BY + HashMap remapping\n3. Validate with existing semantic search tests\n4. Move to Issue #2 (batch fetching optimization)\n",
  "git": {
    "branch": "main",
    "commit": "17d1472e7d434dd45445caaaf24b3ea0ba077768",
    "dirty": true,
    "files_changed": [
      "JULIE_AGENT_INSTRUCTIONS.md"
    ]
  }
}